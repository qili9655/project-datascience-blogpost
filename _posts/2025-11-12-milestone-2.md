---
layout: post
title: "Milestone 2"
date: 2025-11-12
---

<br>
<br>

# Introduction
L'objectif pour ce milestone est de créer des modèles prédictifs de la probabilité que les tirs aboutissent en buts, ou les *expected Goals, xG*, selon diverses caractéristiques telles la position du tir, le type de tir, la situation de l'équipe au moment du tirs, etc.
Nous démontrerons la démarche pour développer 5 modèles de prédiction, et notre ingénierie des caractéristiques pour inclure plus de facteurs pertinents dans la prédiction des *xG*.
L'entraînement de tous les modèles a été effectué avec les données des saisons 2016/17 à 2019/20 et les tests, sur les données de la saison 2020-2021.
<br>

Voici les **modèles prédictifs créés durant ce milestone** et enregistrés dans le registre *Models_IFT6758_2025_A04* (lien ci-dessous) et en tant qu'artéfacts sur la plateforme WandB taggés avec l'alias `best`: <br>
https://wandb.ai/orgs/qi-li-1-universit-de-montr-al-org/registry/Models_IFT6758_2025_A04

- Classifieur de régression logistique avec la distance du filet comme paramètre
- Classifieur de régression logistique avec l'angle comme paramètre
- Classifieur de régression logistique avec la distance et l'angle comme paramètres
- XGBoost (tuned with selected features)
- Méthodes par ensemble
<br>
<br>

# Partie 2: Ingénierie des caractéristiques simples
## Relation des tirs/buts selon leur distance et angle par rapport au filet
Nous commençons d'abord par explorer la relation entre quelques caractéristiques de base potentiellement importantes pour la prédiction des *xG* et le nombre de tirs et de buts.<br>
Voici 3 figures visualisant le nombre de tirs et de buts selon les caractéristiques:<br>
- Distances par rapport au filet
- Angle du tir
- Distance par rapport au *et* angle du tir

![Hist_tirs_ distance](/figures/Hist_tirs_distance.png)
![Hist_tir_angle](/figures/Hist_tirs_angle.png)
![Hist_tirs_distance_angle](/figures/Hist_tirs_distance_angle.png)
<br>
Le premier histogramme révèle une relation très forte entre la distance au filet et la fréquence des tirs ainsi que des buts. Le nombre de tirs augmente en se rapprochant du filet et atteint un maximum autour de 5 à 10 pieds, et décroît avec la distance. La proportion de buts suit la même tendance : les tirs effectués très près du filet génèrent beaucoup plus de buts que ceux provenant de zones plus éloignées. Cette association est logique : plus un joueur est proche du filet, plus la probabilité de marquer augmente.

À l’inverse, la relation entre l’angle du tir et la fréquence des tirs est beaucoup moins marquée. On observe davantage de tirs à des angles très ouverts (0–10 degrés) ainsi qu’entre 20 et 45 degrés, probablement en raison de la géométrie des zones offensives et du positionnement typique des attaquants. Les buts se produisent majoritairement avec des tirs à moins de 40 degrés par rapport au but, tandis que les tirs à des angles plus extrêmes (>50–60°) génèrent très peu de buts. Toutefois, le lien entre l’angle et le nombre de buts reste faible. Ceci démontre que l’angle joue un rôle, mais qu’il est beaucoup moins déterminant que la distance. 

Enfin, la heatmap renforce ces observations. La majorité des tirs se concentrent dans une zone logique : proches du filet (0–50 pieds) et angles modérés (<50 degrés). Une petite densité autour de 60 pieds et 30 degrés apparaît, mais elle est vraisemblablement attribuable à du bruit ou un artefact de binning, puisqu’il n’existe pas de justification tactique ou mécanique pour une concentration réelle de tirs à cet endroit. Dans l’ensemble, la distribution bidimensionnelle confirme que la distance est le facteur dominant dans la génération d’occasions dangereuses, tandis que l’angle est un modulateur secondaire.
<br>
<br>

Ensuite, nous avions validé davantage nos données en examinant s'il y avait des buts provenant d'endroits impossibles. <br>
Voici un histogramme de buts uniquement, classés par distance, avec les événements séparés en filets vides et non vides: 
![Hist_filets_vides_non_vides](/figures/hist_filets_vides_non_vides.png)

La grande majorité des buts observés sont cohérents avec notre connaissance du domaine : ils surviennent presque tous à moins de 60 pieds, c’est-à-dire clairement dans la zone offensive, où la quasi-totalité des buts « normaux » sont habituellement marqués. Même les buts sur filet désert restent majoritairement sous ce seuil de 60 pieds, ce qui est aussi logique puisqu’un tir lointain reste difficile à cadrer malgré l’absence de gardien. Nous observons toutefois quelques buts non-filet-vide à plus de 60 pieds ; ces événements sont très rares et constituent probablement des valeurs aberrantes (coordonnées x/y incorrectes ou mauvais type de tir attribué). Leur faible nombre suggère néanmoins que l’intégrité globale du jeu de données est satisfaisante.
<br>
<br>

# Partie 3 : Modèles de base
## Modèle de régression logistique
Pour commencer le développement de modèles prédictifs, nous avons entraîné un modèle de base: le classifieur `LogisticRegression()`, avec seulement la distance comme paramètre du modèle. La précision de ce classifieur est de 0.091, donc seulement 9.1 % des tirs prédits comme « but » étaient vraiment des buts. <br>
En examinant les prédictions produites par ce modèle, on observe rapidement les limites d'utiliser seulement la précision. La précision est définie comme le rapport entre le nombre de vrais buts correctement prédits et l’ensemble des tirs que le modèle identifie comme buts. Par contre, dans les données en hockey, les classes sont très débalancées, c'est-à-dire que le nombre total de buts est extrêmement faible par rapport au nombre total de tirs. Dans ce contexte, même un modèle performant du point de vue de la capacité de classement des tirs aura tendance à prédire très peu d’événements comme « but », ce qui entraînera toujours une précision faible. La précision dépend beaucoup de la prévalence de l'événement à prédire (buts) en plus de la performance du modèle, et si cet événement est rare, elle restera faible peu importe le modèle. <br>
Par conséquent, la précision (ainsi que le *accuracy*) n’est pas une bonne métrique dans notre contexte de classes débalancées pour évaluer la performance des modèles. Elle est utile dans des contextes où on veut éviter des faux positifs, ce qui n'est pas l'objectif de nos modèles de *xG*. Il faut donc utiliser d’autres métriques mieux adaptées aux données déséquilibrées, notamment l’AUC, les courbes de calibration, le taux de buts par centile ou encore la proportion cumulée de buts capturés.

En tenant en compte les limitations d'un seul modèle et d'une seule métrique de performance, nous avons entraîné 2 autres classificateurs `LogisticRegression()` (un modèle avec l'angle du tir comme paramètre et un autre modèle avec la distance *et* l'angle comme paramètres) et avons inclus davantage de métriques. 

Pour chaque modèle, voici une description de sa performance  et 4 graphiques démontrant les métriques de performance suivantes:
- Courbe de ROC et la métrique AUC
- Taux de buts en fonction du centile de probabilité du tir selon le modèle
- Proportion cumulée de buts en fonction du centile de probabilité selon le modèle
- Diagramme de fiabilité
<br>
<br>

## Classifieur `LogisticRegression` entraîné avec *la distance* 
![Métriques_LR_dist](/figures/Métriques_LR_dist.png)

Valeurs des métriques de performances:
```
model_distance/accuracy:   0.976623197586826
model_distance/auc:        0.7487254246038493
model_distance/f1:         0.13894016176125112
model_distance/precision:  0.09128996692392503
model_distance/recall:     0.2906486941870261
```
- La courbe ROC est bien au-dessus de la diagonale du classificateur aléatoire et l'AUC ~ 0.75, indiquant une bonne performance de classement pour un modèle simple. Plus on se déplace vers le coin supérieur gauche, plus cela confirme que la distance est une information très utile.
- Lorsqu'on trie les tirs selon la probabilité prédite (courbe du taux de buts par centile), on observe que les centiles des probabilités les plus élevées (100-90%) contiennent un taux de buts de ~17%, beaucoup plus qu'aux taux moyens. Le modèle réussit donc à classer les tirs les plus dangereux dans les centiles plus élevés. La courbe des taux de buts diminuent graduellement vers les centiles plus bas, ce qu'on s'attend d'une courbe d'un modèle qui a un bon pouvoir de classement. 
- La courbe de proportion cumulée de buts monte rapidement : les centiles les plus élevés capturent rapidement une grande proportion des buts. Ainsi, une petite fraction des tirs “hautement dangereux" récupère rapidement une large part des buts. Cela appuie davantage que la bonne performance du modèle. 
- La courbe de calibration permet de vérifier si les probabilités prédites par le modèle correspondent aux probabilités réelles observées.
Dans ce graphique, la ligne bleue du modèle est presque confondue avec la diagonale noire (ligne idéale). Elle est tout légèrement sous la diagonale, donc sous-estime légèrement la probabilité réelle des buts. Cela signifie que lorsque le modèle prédit, par exemple, une probabilité de 10 % de marquer, on observe effectivement environ 10% de buts dans ces cas-là. Le modèle est bien calibré, il ne surestime ni ne sous-estime de manière notable les chances de marquer. Toutefois, on remarque que toutes les valeurs sont concentrées en dessous de 0,2, ce qui montre que le modèle attribue généralement de très faibles probabilités de but à l’ensemble des tirs. En résumé, la calibration est bonne, mais la discrimination reste faible.
<br>
<br>

## Classifieur `LogisticRegression` entraîné avec *l'angle*
![Métriques_LR_angle](/figures/Métriques_LR_angle.png)
Valeurs des métriques de performances:
```
model_angle/accuracy:      0.976623197586826
model_angle/auc:           0.6322638528771083
model_angle/f1:            0.07299311439177128
model_angle/precision:     0.04059249921210211
model_angle/recall:        0.3616961527660769
```
- LA courbe ROC est presque collée à la diagonale en pointillé noir, ce qui indique que le modèle ne parvient pas à bien distinguer les tirs qui se terminent par un but de ceux qui ne marquent pas. Autrement dit, la probabilité qu’il attribue à chaque tir est souvent trop proche du hasard. L’angle seul apporte donc très peu d’information discriminante comparé à la distance : deux tirs ayant des angles différents ne sont pas systématiquement associés à des probabilités de but très différentes. La forme quasi linéaire de la courbe montre que le modèle n’apprend qu’un signal extrêmement faible, ce qui se reflète dans une AUC très basse (~0.63) et très proche d’un classifieur aléatoire.
- On observe que les taux de buts sont faibles (max 9% comparé à 17-18% pour les 2 autres modèles) et presque constants d’un centile à l’autre, sans augmentation nette vers les centiles élevés. La courbe a également une plus grande variabilité comparée aux autres modèles (re-augmente au centile 20), ce qui ne concorde pas avec les attentes d'un modèle de bonne performance. Cela signifie que le modèle ne parvient pas à ordonner correctement les tirs selon leur risque réel. Les tirs classés parmi les 10 % les plus probables ne présentent qu’une légère augmentation du taux de but. 
- La courbe orange suit une forme presque linéaire, sans forte courbure vers le haut. Cela signifie que le modèle ne parvient pas à concentrer les buts dans les centiles les plus probables. En d’autres termes, les buts sont répartis de manière presque uniforme sur l’ensemble des tirs, au lieu d’être regroupés dans les tirs à haute probabilité. Ce comportement correspond à ce que l’on attend d’un modèle peu performant : même si l’ordre des tirs est influencé par l’angle, cet ordre ne correspond pas réellement à la qualité des occasions de marquer. À l’inverse, un modèle performant afficherait une courbe très convexe (un faible pourcentage de tirs regroupe la majorité des buts).
- La courbe bleue est quasiment confondue avec la diagonale, ce qui signifie que les probabilités prédites sont globalement bien calibrées. Cependant, comme la majorité des probabilités sont très faibles (0.0–0.1), le modèle reste bien calibré mais peu discriminant. Il prédit juste, mais il prédit surtout des valeurs très faibles pour presque tous les tirs. Cette apparence de calibration est donc moins une preuve de qualité qu’une conséquence du manque d’information contenue dans l’angle seul : le modèle prédit toujours des valeurs faibles et reste dans une zone "sécuritaire" où il est difficile de mal calibrer ses prédictions. 

## Classifieur `LogisticRegression` entraîné avec *la distance et l'angle*
![Métriques_LR_angle](/figures/Métriques_LR_dist_angle.png)
Valeurs des métriques de performances:
```
model_distanceangle/accuracy:   0.976623197586826
model_distanceangle/auc:        0.8098626300211255
model_distanceangle/f1:         0.1617653193175656
model_distanceangle/precision:  0.11506735505311258
model_distanceangle/recall:     0.2722549845549003
```

- La courbe ROC est nettement au-dessus de la diagonale, ce qui indique une bonne capacité du modèle à distinguer les tirs qui deviennent des buts de ceux qui ne le deviennent pas. Le modèle présente donc une meilleure séparation que les modèles basés sur une seule variable (distance ou angle seul). En pratique, cela signifie que le modèle attribue des probabilités plus élevées aux tirs réellement dangereux. L’intégration de la distance et de l’angle améliore la discrimination des tirs réussis vs non-réussis, car ces deux caractéristiques capturent des aspects complémentaires de la dangerosité d’un tir : proximité du filet et angle de tir plus central. La meilleure performance du modèle se reflète dans une valeur d'AUC supérieure et une courbe davantage recourbée vers le coin supérieur gauche.
- On observe une forte décroissance du taux de buts entre les centiles les plus élevés et les plus faibles. Les tirs classés parmi les 10 % les plus probables ont un taux de buts d’environ 17 %, tandis que les plus faibles se situent autour de 2 %. Cela montre que le modèle classe correctement les tirs selon leur dangerosité : plus la probabilité prédite est grande, plus le tir a réellement de chances de se transformer en but. Par rapport aux modèles utilisant uniquement la distance ou uniquement l’angle, la pente est ici plus marquée, ce qui indique une meilleure séparation entre tirs dangereux et non dangereux. Cette courbe confirme que l’ajout de l’angle renforce la pertinence du classement produit par le modèle. 
- La courbe monte rapidement : en ne conservant qu’une petite portion des tirs les mieux classés, on capture déjà la majorité des buts. Par exemple, environ 40 % des tirs prédits comme les plus “dangereux” contiennent près de 80 % des buts. Ce résultat confirme que le modèle identifie efficacement les occasions à haut risque et qu’il est utile pour évaluer la qualité des tirs. La combinaison distance + angle produit ici une courbe plus convexe que les modèles à une seule variable, ce qui montre une capacité accrue à concentrer les buts dans les centiles les plus élevés. 
- La courbe bleue est très proche de la diagonale idéale, ce qui signifie que les probabilités prédites reflètent fidèlement les probabilités réelles observées. Autrement dit, si le modèle prédit 20 % de chances de marquer, on observe environ 20 % de buts dans ces cas. Le modèle est donc bien calibré, en plus d’être performant. 
<br>
<br>

# Partie 4 : Ingénierie des caractéristiques avancées
Afin d'optimiser la performance de nos modèles, nous avons ajouté ajouté plus de caractéristiques à nos dataframes et effectué des ingénieries de caractéristiques dans le but d'entraîner nos modèles avec davantage de paramètres et des paramètres plus pertinents. <br>
<br>
Voici une liste exhaustive de toutes les caractéristiques que nous comptons utiliser, répertoriés avec leur nom dans le dataframe des données d'entraînement.
<br>

```python
    df_clean = df[[
        # Coordonnées actuelles
        "xCoord", "yCoord",
        # Géométrie
        "distance_net", "angle_net",
        # Target et empty net
        "is_goal", "empty_net",
        # Temps
        "game_seconds", "game_period",
        # Type de tir
        "shot_type",
        # Coordonnées événement précédent (REQUIS par le devoir)
        "prev_xCoord", "prev_yCoord",
        # Temps écoulé depuis événement précédent (REQUIS par le devoir)
        "delta_t",
        # Features dérivées
        "is_rebound", "change_in_angle", "shot_speed", "distance_prev_event",
        # Power-play features (BONUS 5%)
        "time_since_powerplay_start", "friendly_skaters", "opponent_skaters",
        # Métadonnées
        "season", "teamAbbr", "idGame",
        # Contexte
        "prev_event", "prev_team"
    ]].copy()
```

## Liste de caractéristiques 

| nom_de_colonne        | Description                                                                                  |
|-----------------------|------------------------------------------------------------------------------------------------------------------------|
| `season`                | Saison NHL de l’événement (ex. 2019, 2020…).                                                                           |
| `teamAbbr`              | Abréviation de l’équipe du tireur (ex. MTL, TOR).                                                                     |
| `idGame`                | Identifiant unique du match.                                                                                           |
| `xCoord`                | Coordonnée X de l’emplacement du tir sur la patinoire.                                                                |
| `yCoord`                | Coordonnée Y de l’emplacement du tir sur la patinoire.                                                                |
| `distance_net`          | Distance (en pieds) entre le tireur et le centre du filet au moment du tir.                                           |
| `angle_net`             | Angle du tir par rapport au centre du filet (en degrés).                                                               |
| `is_goal`               | Variable cible : 1 si le tir est un but, 0 sinon.                                                                     |
| `empty_net`             | 1 si le but est marqué dans un filet désert, 0 sinon     |
| `game_seconds`          | Nombre de secondes écoulées depuis le début du match (périodes incluses).                                             |
| `game_period`           | Période de jeu actuelle (1, 2, 3 ou prolongation).                                                                    |
| `shot_type`             | Type de tir (ex. wrist, snap, slap, tip-in).                                                                          |
| `prev_event`            | Type de l’événement immédiatement précédent (ex. shot-on-goal, goal, pass…).                                          |
| `prev_team`             | Abréviation de l’équipe ayant effectué l’événement précédent.                                                         |
| `prev_xCoord`         | Coordonnée X de l’événement immédiatement précédent dans le même match (tir, passe, récupération, etc.).                   |
| `prev_yCoord`         | Coordonnée Y de l’événement immédiatement précédent dans le même match.                                                      |
| `prev_time`           | Temps du dernier événement dans la période, en format "MM:SS".                                                               |
| `time_sec`            | Temps du tir converti en secondes depuis le début de la période (par ex. "03:42" → 222 secondes).                           |
| `prev_time_sec`       | Temps de l’événement précédent converti en secondes depuis le début de la période.                                           |
| `delta_t`             | Différence en secondes entre l’événement précédent et le tir actuel (time_sec - prev_time_sec).                             |
| `distance_prev_event`   | Distance (en pieds) entre l’événement précédent et le tir actuel.                                                     |
| `prev_angle_net`      | Angle du dernier tir par rapport au filet (existe uniquement si l’événement précédent était un tir ou un but).              |
| `is_rebound`            | 1 si l’événement précédent du même match était un tir (rebond), 0 sinon.                                              |
| `change_in_angle`       | Changement d’angle entre le tir précédent et le tir actuel (0 si pas un rebond).                                      |
| `shot_speed`            | « Vitesse » approximative avant le tir, définie comme distance_prev_event / delta_t.                                   |
| `time_since_powerplay_start` | Temps écoulé (en secondes) depuis le début du dernier avantage numérique actif. 0 si non en power-play ou si le tir est en désavantage numérique. |
| `friendly_skaters`           | Nombre de patineurs (hors gardien) pour l’équipe qui tire, après intégration des pénalités actives.      |
| `opponent_skaters`           | Nombre de patineurs (hors gardien) pour l’équipe adverse au moment du tir.                               |
| `prev_event`                 | Type de l’événement immédiatement précédent dans ce même match.                                          |
| `prev_team`                  | Équipe ayant réalisé l’événement précédent.                                                              |

<br>

## Exemple de dataframe 
Comme exemple, nous avons produit un dataframe pour le match Winnipeg vs Washington du 12 mars 2018 avec les caractéristiques ci-hautes. <br>
Voici le lien URL menant à l'expérience (*run*) stockant l'artéfact du dataframe:<br>
https://wandb.ai/qi-li-1-universit-de-montr-al/IFT6758-2025/runs/y53wvgcz?nw=nwuserqili1
<br>
<br>

# Partie 5 : XGBoost
Pour explorer des modèles prédictifs plus avancés, nous avons créé différentes version du modèle XGBoost. 
<br>

Pour entraîner et évaluer ces modèles, nous avons utilisé les fonctions `generate_train_val_test_datasets()` (`feature_ingenering.py`) et `load_and_prepare_data()` (`model_xgboost.py`), qui chargent les données des saisons 2016/17 à 2019/20 pour constituer les ensembles d’entraînement et de validation (80%, soit 193 227 tirs, en entraînement, et 20%, soit 48 307 tirs en validation), et réservent complètement la saison 2020/21 (57 734 tirs) comme ensemble de test indépendant. Les variables catégorielles (notamment `shot_type`) sont automatiquement encodées en one-hot, les valeurs manquantes sont gérées, et les sorties consistent en des matrices prêtes pour la modélisation. Comme attendu en hockey, la proportion de buts demeure faible (~9–10 %), créant un déséquilibre marqué (≈ 1 but pour 9,6 tirs non convertis), contexte qui peut favoriser un biais des modèles vers la classe majoritaire. <br>
Voici un aperçu de la distribution des tirs et des buts dans les 3 ensembles données:

![Distribution_train_val_test](/figures/Distribution_train_val_test.png)
<br>
<br>

## Classifieur XGBoost entraîné avec *la distance par rapport au filet et l'angle du tir*
Nous avons fait un premier essai simple du XGBoost avec les caractéristiques simples de la partie 3, c'est-à-dire la distance du tir par rapport au filet et l'angle du tir. Voici les courbes contenant les mêmes métriques de performance que le classifieur simple de `LogisticRegression()`: 

![Métriques_XGB_Baseline](/figures/Métriques_XGB_Baseline.png)
| Metric     | Value    |
|------------|----------|
| AUC        | 0.6997   |
| Accuracy   | 0.90511  |
| Precision  | 0.47059  |
| Recall     | 0.00437  |
| F1-score   | 0.00866  |

Le modèle simple de régression logistique a obtenu des performances plus solides que celle du XGBoost, atteignant une AUC de 0.81, avec une sensibilité notablement plus élevée (rappel ≈ 0.27). XGBoost génère une AUC d’environ 0.74 et demeure moins performant sur toutes les métriques liées aux buts, notamment en rappel et en calibration. Cette différence suggère que, avec une faible dimension de caractéristiques (seulement 2: distance, angle), la régression logistique capture plus efficacement la structure quasi-linéaire de la relation entre position du tir et probabilité de but, tandis que XGBoost, sans ajustement du déséquilibre (ex. *scale_pos_weight*), a davantage tendance à se rabattre sur la classe majoritaire. <br>
<br>
Les quatre graphiques obtenus sont cohérents avec les métriques du modèle XGBoost baseline. L’AUC-ROC d’environ 0.70 se reflète dans une courbe ROC au-dessus du classifieur aléatoire, indiquant une capacité raisonnable à ordonner les tirs selon leur dangerosité. L’accuracy élevée (~90 %) provient du fort déséquilibre des classes, car le modèle prédit presque toujours “no goal”. Cela explique la précision modérée (le peu de tirs prédits comme buts sont relativement corrects), mais surtout le rappel extrêmement faible (le modèle ne détecte presque aucun but réel), d’où un F1-score proche de zéro. Les courbes de calibration et les graphiques de distribution confirment ce comportement conservateur : le modèle classe globalement bien les tirs, mais ne génère que très rarement des probabilités suffisamment élevées pour identifier un but.<br>
<br>
Pour appuyer ces points, voici la matrice de confusion et les distributions de probabilité associées à ce modèle. 
<br>
![q1_confusion_matrix_baseline](/figures/q1_confusion_matrix_baseline.png) 
![q1_probability_distribution_baseline](/figures/q1_probability_distribution_baseline.png) 

La matrice de confusion met en évidence une très forte asymétrie dans les prédictions : bien que le modèle atteigne une exactitude élevée (≈ 90 %), il prédit presque toujours la classe « non-but », ce qui entraîne un rappel extrêmement faible pour les buts (< 0,5 %). Ce comportement est confirmé par la distribution des probabilités prédites : la grande majorité des tirs reçoivent une probabilité inférieure à 0,15, et seules quelques observations dépassent 0,30. Si cette distribution reflète une calibration globale raisonnable — la probabilité moyenne prédite étant très proche du taux réel de buts — elle révèle aussi un modèle trop « conservateur », qui « n'ose » pas attribuer des probabilités élevées même aux tirs réellement dangereux. Cette combinaison d’un fort déséquilibre dans les prédictions et d’une faible dispersion des probabilités explique simultanément la bonne AUC (capacité d’ordonner les tirs) et la très faible capacité de détection des buts.
<br>

## Importance des caractéristiques
XGBoost a une fonction qui nous donne l'importance des caractéristiques selon:<br>
- L'amélioration de la fonction de perte que chaque caractéristique amène dans la tâche de classification
- Le nombre d'échantillons impacté par cette caractéristique
- Le nombre de fois que cette caractéristique a été utilisées dans la tâche <br>
Elle nous informe donc sur la contribution de chaque caractéristique à mieux classer les entrées. En utilisant cette fonction (`model.feature_importance_`) sur les deux caractéristiques ci-haut, nous obtenons la même conclusion que dans la partie 3, c'est-à-dire que la distance est une caractéristique beaucoup plus importante que l'angle.

![Importance_feat_baseline](/figures/Importance_feat_baseline.png)


## Classifieur XGBoost entraîné avec *toutes les caractéristiques de la partie 4* et meilleurs hyperparamètres
Par la suite, pour optimiser notre XGBoost, nous avons utilisé la totalité des 18 caractéristiques créées dans la partie 4. Avec la fonction ous avons également effectué une recherche de la meilleure combinaison d'hyperparamètres parmi la liste ci-dessous par la méthode `RandomizedSearch`. Cette approche présente plusieurs avantages comparée au `GridSearchCV`: elle explore un espace d’hyperparamètres potentiellement très large sans tester toutes les combinaisons possible, réduisant le coût computationnel. De plus, elle permet d’échantillonner aléatoirement des configurations diverses et souvent plus efficaces, améliorant les chances de découvrir une bonne région de l’espace de recherche. Elle est donc bien adaptée aux modèles complexes comme XGBoost, où l’espace d’hyperparamètres peut être très vaste.
<br>

Liste d'hyperparamètres recherchés: 
```python
param_grid = {
    'max_depth': [3, 5, 7],  # Profondeur maximale des arbres (contrôle la complexité: 3=simple, 7=complexe)
    'learning_rate': [0.01, 0.05, 0.1],  # Taux d'apprentissage (*shrinkage*)
    'n_estimators': [100, 200, 300],  # Nombre d'arbres (*boosting iterations*)
    'min_child_weight': [1, 3, 5],  # Régularisation: poids minimum d'une feuille
    'subsample': [0.8, 0.9, 1.0],  # % d'échantillons par arbre
    'colsample_bytree': [0.8, 0.9, 1.0],  # % de features par arbre
    'scale_pos_weight': [scale_pos_weight, scale_pos_weight*0.5]  # Poids de la classe positive (pour gérer le déséquilibre)
}
```

Ceci donne l'espace de recherche avec les caractéristiques suivantes: <br>
- Combinaisons totales recherchées: 1,458
- Méthode: RandomizedSearchCV (100 essais)
- Validation croisée: 5-fold
- Métrique d'optimisation: ROC AUC
<br>

En entraînant le nouveau modèle avec la stratégie de recherche ci-dessus, nous obtenons la meilleure combinaison ci-dessous:<br>
- max_depth: 5
- learning_rate: 0.05
- n_estimators: 200
- subsample: 0.9
- n_estimators: 200
- subsample: 0.9
- colsample_bytree: 0.8
- min_child_weight: 3
- scale_pos_weight: 4.779426229508197
- colsample_bytree: 0.8
- min_child_weight: 3
- scale_pos_weight: 4.779426229508197
<br>

Et une amélioration des métriques de performance: <br>
| Métrique de performance       | Valeur                                                                                  |
|-----------------------|------------------------------------------------------------------------------------------------------------------------|
| AUC-ROC            | 0.7601                                                                          |
| Accuracy           | 0.8337                                                                        |
| Precision          | 0.2570                                                                       |
| Recall             | 0.3981                                                                       |
| F1-Score           | 0.3123                                                                       |

Amélioration AUC vs Baseline: +8.63%
<br>
<br>

Nous avons aussi créé des courbes des 4 métriques de performance du modèle avec la meilleure combinaison d'hyperparamètres. Ces courbes seront affichées à la fin de cette section, avec l'ensemble des courbes des modèles XGBoost qui seront générés. <br>
Voici quelques figures démontrant l'effet des ajustements d'hyperparamètres sur les métriques de performance du modèle.
![q2_threshold_analysis](/figures/q2_threshold_analysis.png)
<br>
Et également la matrice de confusion et les distributions de probabilités prédites, optimisées par rapport au modèle XGBoost de base ci-haut. Le modèle identifie maintenant plus de vrais positifs (est plus sensible, tel que démontré par le *Recall* amélioré) et donne en moyenne des scores plus hauts de buts pour chaque événement. <br>
![q2_confusion_matrix_tuned](/figures/q2_confusion_matrix_tuned.png)
![q2_probability_distribution_tuned](/figures/q2_probability_distribution_tuned.png) 
<br>

## Classifieur XGBoost entraîné avec des *caractéristiques sélectionnées*
Pour réduire la complexité du modèle et sélectionner les caractéristiques les plus pertinentes, nous avons utilisé 4 approches pour identifier les variables les plus prédictives du modèle XGBoost : <br>
<br>

### Méthodes de sélection de caractéristiques utilisées
Pour réduire la complexité du modèle et sélectionner les caractéristiques les plus pertinentes, nous avons utilisé 4 approches pour identifier les variables les plus prédictives du modèle XGBoost : <br>
<br>

1.	Méthode embarquée (**`Feature Importance`** de XGBoost) – s’appuie sur l’importance interne attribuée par le modèle à chaque variable lors de l’entraînement. Cette méthode met en évidence les caractéristiques influentes comme distance_net, angle_net, et shot_type_slap, offrant un bon compromis entre performance (AUC = 0.7355) et interprétabilité. <br>
![top_ten_feat_by_importance](/figures/top_ten_feat_by_importance.png)
<br>

2.	Méthode statistique (*Mutual Information*) – évalue la dépendance entre chaque variable et la cible (is_goal) sans supposer de relation linéaire. Les dix variables sélectionnées (par ex. xCoord, yCoord, distance_net, angle_net, shot_speed, friendly_skaters) expliquent une part importante de la variance (AUC = 0.7207). <br>
![q3_statistical_mutual_info](/figures/q3_statistical_mutual_info.png)

3.	Méthode wrapper (Recursive Feature Elimination, RFE) – consiste à entraîner plusieurs modèles en supprimant progressivement les variables les moins utiles. Elle a retenu 13 caractéristiques optimales, incluant yCoord, distance_net, angle_net, shot_speed et empty_net, et a obtenu la meilleure performance globale (AUC = 0.7366).
![q3_rfe_analysis](/figures/q3_rfe_analysis.png)

4.	Méthode embarquée par régularisation L1 (Lasso) – privilégie la parcimonie en forçant les coefficients peu significatifs vers zéro. Le modèle a sélectionné 10 variables dont distance_net, angle_net, empty_net et shot_speed, avec une performance stable (AUC = 0.7257).
![q3_l1_regularization](/figures/q3_l1_regularization.png)

Dans l’ensemble, les variables distance_net et angle_net sont apparues dans toutes les méthodes, confirmant leur rôle central dans la prédiction des buts. <br>
Voici les caractéristiques sélectionnées par chaque méthode: <br>
| Méthode        | Caractéristiques sélectionnées                                                                                  |
|----------------|-----------------------------------------------------------------------------------------------------------------|
| Feature Importance (XGBoost) - embarquée | distance_net, shot_type_slap, shot_type_backhand, angle_net, game_period, shot_speed, yCoord, xCoord, delta_t, opponent_skaters |
| Mutual Information - statistique           | xCoord, yCoord, distance_net, angle_net, game_seconds, delta_t, is_rebound, shot_speed, friendly_skaters, shot_type_wrap-around                                                                        |
| Recursive Feature Elimination (RFE) - wrapper          | yCoord, distance_net, angle_net, empty_net, game_seconds, delta_t, shot_speed, time_since_powerplay_start, shot_type_backhand, shot_type_deflected, shot_type_slap, shot_type_wrap-around, shot_type_wrist                                                                     |
| L1 Regularization (Lasso) - embarquée         | distance_net, angle_net, empty_net, game_seconds, is_rebound, shot_speed, distance_prev_event, opponent_skaters, shot_type_backhand,shot_type_wrap-around                                                                         |
<br>

Et les AUC de chaque méthode:<br>
| Méthode                                   | AUC Validation | Nb de Features |
|-------------------------------------------|----------------|----------------|
| Feature Importance (XGBoost) – Embarquée  | 0.7355         | 10             |
| Mutual Information – Statistique          | 0.7207         | 10             |
| Recursive Feature Elimination (RFE) – Wrapper | 0.7366     | 13             |
| L1 Regularization (Lasso) – Embarquée     | 0.7257         | 10             |
<br>
Nous avons choisi d'utiliser la méthode `Feature Importance` de XGBoost, car elle offre un bon compromis avec sa valeur de AUC à 0.7355 et le petit nombre de caractéristiques nécessaires (10), comparé à RFE qui a une AUC similaire mais plus de caractéristiques requises. 
<br>

### Analyses SHAP
Nous avons aussi effectué des analyses SHAP pour trier et interpréter les caractéristiques les plus importantes. Les caractéristiques les plus importantes sélectionnées par le SHAP correspondent globalement à celles de la méthode `Feature Importance`:
![shap_importance_bar](/figures/shap_importance_bar.png)

<br>

### Métriques de performance du modèle 
Voici les métriques de performance d'un modèle XGBoost entraîné avec les 10 caractéristiques les plus importantes selon le `Feature Importance`de XGBoost, et la meilleure combinaison d'hyperparamètres trouvés à la section précédente:<br>
| Métrique de performance       | Valeur                                                  |
|-------------------------------|---------------------------------------------------------|
| AUC-ROC                       | 0.7452                                                  |
| Accuracy                      | 0.8310                                                  |
| Precision                     | 0.2492                                                  |
| Recall                        | 0.3883                                                  |
| F1-Score                      | 0.3036                                                  |
<br>

**TRADE-OFF COMPLEXITE/PERFORMANCE**:<br>
  Features:    10/24 (-58.3%)<br>
  Performance: 98.04% de l'AUC max<br>
  Perte d'AUC: 0.0149<br>
<br>

Notre approche a permis d’isoler les dimensions les plus déterminantes du modèle, notamment la géométrie du tir (distance_net, angle_net, xCoord, yCoord), certains types de tirs (wrist, slap, snap, etc.) ainsi que des éléments de contexte temporel et de jeu comme game_seconds ou is_rebound. La réduction finale à environ dix caractéristiques offre un bon compromis : un modèle plus simple (≈58% de variables en moins), plus rapide, robuste (moins de risque de *overfitting*) et inteprétable, tout en conservant 98% de la performance AUC du modèle complet. Cette réduction peut engendrer une très légère baisse de performance (~0.01 AUC), mais élimine surtout des caractéristiques redondantes, peu informatives ou bruitées, ce qui améliore la lisibilité et la stabilité globale du modèle. **Nous retenons donc cette version de XGBoost comme la meilleure pour utilisation dans les prochaines étapes.** <br>


## Courbes de métriques de performance: ensemble des modèles XGBoost
Pour conclure la section, voici les 4 courbes de tous les modèles XGBoost générés ci-haut.
![ROC_curves_analyis](/figures/roc_curves_analysis.png)
![xgboost_cumulative_goals](/figures/xgboost_cumulative_goals.png)
![xgboost_goal_rate_percentile](/figures/xgboost_goal_rate_percentile.png)
![xgboost_calibration_curves](/figures/xgboost_calibration_curves.png)

Nous sommes conscients que 3 dernières courbes ne sont pas représentatrices de la performance du modèle (problème de code...). Pour au moins avoir une idée de la calibration, nous avons calculé le score de Brier pour nos 3 modèles de XGBoost, qui sont de qualité modérée à bonne:
- XGBoost baseline: 0.13
- XGBoost *all features tuned*: 0.09
- XGBoost *selected features tuned*: 0.12
<br>
<br>

# Partie 6 : Tentatives de modèle plus performant

Dans la continuité du développement du modèle XGBoost, nous avons exploré d’autres modèles susceptibles d’améliorer les performances prédictives. Parmi l’ensemble des algorithmes étudiés jusqu’à présent, **XGBoost** nous semble le plus "performant", grâce à sa capacité de *gradient boosting* avec beaucoup d'ajustements possibles (régularisation pour limiter le surapprentissage, *shrinkage*, construction de l'arbre de façon flexible, gestion des valeurs manquantes, gestion du déséquilibre de classes) . Ce modèle offre en outre une grande flexibilité dans l’optimisation de ses hyperparamètres et une interprétabilité accrue via l’analyse des importances de variables. Par contre, ce modèle plus complexe peut avoir une moins bonne performance comparée aux méthodes plus simples (telles que démontré ci-haut: AUC similaires à la régression logistique) si les hyperparamètres ne sont pas bien ajustés. 
<br>

Afin de comparer et valider notre impression, nous avons testé plusieurs méthodes par ensemble présentant chacune des avantages:
- AdaBoost renforce itérativement des classifieurs faibles (arbres de décision simples) en accordant davantage de poids aux observations mal classées, ce qui en fait un modèle performant et interprétable pour des données de taille moyenne.
- Random Forest, qui agrège un grand nombre d’arbres construits sur des sous-échantillons aléatoires de données et de variables, est particulièrement robuste face au bruit et réduit la variance au prix d’une légère perte en biais.
- Les méthodes par ensembles (telles un VotingClassifier()) peuvent combiner la stabilité de plusieurs modèles de base et à améliorer la généralisabilité des prédictions.<br>
<br>

## Modèles moins performants
En parallèle, nous avons aussi testé des modèles plus simples tels que le Perceptron (AUC ≈ 0.55) et le K plus proches voisins (KNN) (AUC ≈ 0.68), mais leurs performances se sont révélées insuffisantes. Le perceptron, modèle linéaire sans profondeur ni mécanisme d’ensemble, ne capture pas la complexité non linéaire des interactions entre variables. Le KNN est trop fortement sensible à la distribution spatiale et à la normalisation des données, limitant sa capacité de généraliser sur un espace de caractéristiques hétérogènes comme celles de notre dataset de hockey.

## AdaBoot
Le modèle AdaBoost a ensuite été construit à partir d’un classifieur de base de type `DecisionTreeClassifier`, dont les hyperparamètres ont été optimisés par un *RandomizedSearchCV* sur un espace de paramètres suivant:
```python
param_grid = {
param_grid = {
    "n_estimators": [100, 200, 300],              # nb of weak learners (trees) in the ensemble
    "learning_rate": [0.01, 0.05, 0.1, 0.2],      # weight applied to each tree’s contribution (lower = slower learning, more stable)
    "estimator__max_depth": [1, 2, 3],            # max depth of each decision tree (controls model complexity)
    "estimator__min_samples_split": [2, 5, 10],   # minimum samples required to split an internal node
    "estimator__min_samples_leaf": [1, 2, 4],     # minimum samples required at each leaf node (prevents overfitting)
    "estimator__class_weight": [                  # handles class imbalance in training
        None,                                     # no weighting (default)
        "balanced",                               # automatically weight classes inversely to frequency
        {"balanced": None}                        # placeholder/custom weight dictionary (not used here)
    ]
}
```
Sur l’ensemble de validation, le modèle optimisé a montré une AUC de 0.75 et une précision globale d’environ 90 %, témoignant d’une bonne capacité discriminante mais d’un déséquilibre marqué entre classes, comme l’indiquent les faibles valeurs de rappel et de F1-score.
![Métriques_AdaBoost](/figures/Métriques_AdaBoost.png)
<br>

| Métrique de performance       | Valeur                                                  |
|-------------------------------|---------------------------------------------------------|
| AUC-ROC                       | 0.7548                                                  |
| Accuracy                      | 0.9054                                                  |
| Precision                     | 0.5352                                                  |
| Recall                        | 0.0208                                                  |
| F1-Score                      | 0.0400                                                  |
<br>

Pour explorer la robustesse des variables explicatives, plusieurs approches de sélection de caractéristiques inspirées du pipeline XGBoost ont été comparées : l’importance interne des variables, la sélection par information mutuelle, l’élimination récursive des caractéristiques (RFE), la régularisation L1 et la sélection séquentielle avant (SFS). Les performances globales se sont avérées proches entre méthodes, avec des AUC comprises entre 0.73 et 0.75, la RFE (AUC = 0.754) et la SFS avant (AUC = 0.752) offrant les meilleurs compromis. L’analyse des importances montre que les variables distance_net, delta_t et game_seconds dominent nettement la contribution au modèle, reflétant leur rôle déterminant dans la probabilité de but. Globalement, le modèle AdaBoost présente une calibration satisfaisante dans les faibles probabilités et une stabilité acceptable après sélection de variables. 
<br>
<br>

## Random Forest

Par la suite, nous avons développé un modèle Random Forest, dont un premier modèle de base entraîné avec une pondération équilibrée des classes pour corriger le déséquilibre entre tirs réussis et non réussis:<br>
```python
rf = RandomForestClassifier(
    n_estimators=300,          # nb of trees in the forest (larger = more robust but slower)
    class_weight="balanced",   # automatically weight classes inversely to their frequency
    random_state=42,           # reproducibility of results
    n_jobs=-1                  # use all available CPU cores for parallel computation
)
```

Un `RandomizedSearchCV` a ensuite permis d’explorer la grille d'hyperparamètres ci-dessous:
```python
param_grid = {
    "n_estimators": [100, 200, 300],          # number of trees to train in the forest
    "max_depth": [None, 10, 20, 30, 40],      # maximum depth of each tree (None = fully grown)
    "min_samples_split": [2, 5, 10, 20],      # minimum samples required to split a node
    "min_samples_leaf": [1, 2, 4, 8],         # minimum samples required at each leaf
    "max_features": ["sqrt", "log2", None],   # number of features considered when splitting a node
    "bootstrap": [True, False]                # whether to use bootstrap samples for each tree
}
```

Également, une analyse d’importance des variables (basée sur l’indice de Gini) a permis d’identifier les dix caractéristiques les plus influentes. De plus, les même stratégies de sélection de caractéristiques que celles utilisées pour XGBoost et AdaBoost a été appliquée : information mutuelle, élimination récursive des caractéristiques (RFE), régularisation L1 et sélection séquentielle avant. Voici les graphiques de métriques de performance pour le meilleur modèle (*tuned with all features*):
![Métriques_RF](/figures/Métriques_RF.png)
<br>

| Métrique de performance       | Valeur                                                  |
|-------------------------------|---------------------------------------------------------|
| AUC-ROC                       | 0.7529                                                  |
| Accuracy                      | 0.6834                                                  |
| Precision                     | 0.1839                                                  |
| Recall                        | 0.6800                                                  |
| F1-Score                      | 0.2895                                                  |
<br>

Le modèle Random Forest présente une bonne capacité de discrimination avec une AUC-ROC de 0,75, indiquant une performance robuste pour distinguer les tirs qui mènent à un but de ceux qui n’y mènent pas. La précision (≈0,18) reste modeste — le modèle génère encore de faux positifs — mais la rappel élevé (≈0,68) montre qu’il capture efficacement la majorité des buts réels. Après sélection de variables, les performances demeurent stables (AUC entre 0,74 et 0,75), démontrant une robustesse du modèle même avec un nombre réduit de caractéristiques. Globalement, ces résultats suggèrent que Random Forest équilibre convenablement la généralisation et la complexité du jeu de données, tout en offrant une base solide pour une intégration dans un ensemble de modèles (ensemble learning). <br>
Comparé au AdaBoost, Random Forest est globalement supérieur: son AUC est comparable à celui d’AdaBoost, mais il atteint une sensibilité (recall) et un F1-score beaucoup plus élevés. 

## Méthode d'ensemble `Voting Classifier`
Comme tentative finale, nous avons construit une méthode d’ensemble par vote pondéré (soft voting) combinant plusieurs modèles issus des étapes précédentes d’expérimentation et les mêmes stratégies de sélection de caractéristiques. L’ensemble comprend les modèles avec les métriques les plus performantes ainsi que notre modèle KNN pour enrichir la diversité des prédictions. La combinaison finale vise ainsi à tirer parti des forces spécifiques de chaque approche :
- Régression logistique (distance + angle): modèle linéaire interprétable et bien calibré.
- Random Forest (*all features tuned*): réduction de variance et robustesse aux outliers.
- XGBoost (*all features tuned*): modélisation fine des interactions non linéaires et gestion efficace du déséquilibre de classes.
- KNN (*all features tuned*): approche locale capturant des relations fines entre observations proches.
<br>

La combinaison d'hyperparamètres suivante a été recherchée:
```python
# 2. Hyperparameter search grid (on ensemble internal RF + XGB)
param_grid = {
    "rf__n_estimators": [200, 300, 400],
    "rf__max_depth": [10, 20, None],
    "xgb__max_depth": [3, 5, 7],
    "xgb__learning_rate": [0.01, 0.05, 0.1],
    "xgb__n_estimators": [200, 300],
}
```
Voici les graphiques de métriques de performance pour le meilleur modèle (*tuned with all features*):
![Métriques_Ensemble](/figures/Métriques_Ensemble.png)
<br>

| Métrique de performance       | Valeur                                                  |
|-------------------------------|---------------------------------------------------------|
| AUC-ROC                       | 0.7592                                                  |
| Accuracy                      | 0.8981                                                  |
| Precision                     | 0.3937                                                  |
| Recall                        | 0.1376                                                  |
| F1-Score                      | 0.2040                                                  |

L’ensemble affiche de solides performances globales, avec un AUC de 0,7592 et une accuracy de 0,8981, ce qui traduit une excellente capacité de discrimination entre tirs réussis et non réussis, tout en conservant une calibration correcte. Les hyperparamètres optimaux montrent une forêt dense mais contrôlée (n_estimators=200, max_depth=20) et un XGBoost régularisé (max_depth=3, learning_rate=0.1), indiquant un compromis équilibré entre complexité et généralisation. Les courbes de ROC, taux de buts par centile et proportion cumulée des buts sont de bones qualité. Cependant, la courbe de calibration reste en dessous de la diagonale idéale : le modèle sous-estime légèrement les probabilités réelles de but (signe d’un biais conservateur). 

## Meilleurs modèles finaux

Pour conclure, voici un tableau résumé des métriques de tous les modèles développés à date:
| **Modèle** | **AUC-ROC** | **Accuracy** | **Précision** | **Rappel** | **F1-Score** | **Commentaire global** |
|-------------|-------------|---------------|----------------|-------------|----------------|----------------|
| **LR (angle)** | 0.632 | 0.977 | 0.041 | 0.362 | 0.073 | Très faible pouvoir discriminant (AUC < 0.65) ; modèle non informatif. |
| **LR (distance)** | 0.749 | 0.977 | 0.091 | 0.291 | 0.139 | Baseline correcte ; la distance seule reste un bon prédicteur. |
| **LR (distance + angle)** | **0.810** | **0.977** | 0.115 | 0.272 | 0.162 | Excellent compromis simplicité–performance ; calibration quasi parfaite. |
| **AdaBoost** | 0.755 | **0.905** | **0.535** | 0.021 | 0.040 | Très précis sur les cas positifs prédits, mais très faible rappel. |
| **Random Forest** | 0.753 | 0.683 | 0.184 | **0.680** | 0.290 | Excellente sensibilité, capture la majorité des buts au prix de nombreux faux positifs. |
| **XGBoost** | 0.745 | 0.831 | 0.249 | 0.388 | 0.304 | Bon équilibre entre précision et rappel, robuste. |
| **Ensemble (RF + XGB)** | **0.759** | 0.895 | 0.364 | 0.146 | 0.209 | Bon compromis entre précision et rappel, AUC stable et calibration correcte. |

En résumé **(modèles sélectionnés pour le test dataset en gras)**:
- **Régressions logistiques**: très bien calibrées et interprétables: AUC du modèle distance + angle surpasse toutes les variantes linéaires, bon comme modèle de référence pour la validation 
- **XGBoost**: meilleur compromis, robuste et stable.
- Random Forest est très sensible (haut rappel, peu précis): utile pour détection exhaustive.
- AdaBoost est très précis mais très faible rappel: bon pour filtrage de tirs “quasi sûrs”.
- **Méthode d’ensemble**: légère amélioration d’AUC (0.759), meilleure calibration et stabilité globale parmi les 3 testés dans cette partie: **sélectionné comme meilleur modèle pour le test final**
<br>
<br>

# Partie 7 

Pour cette dernière partie, nous testons les 5 modèles les plus performants sur les ensembles de test suivants:
- Saisons régulières 2020/2021
- Séries éliminatoires 2020/2021

Voici les courbes et les métriques des 5 modèles sur les 2 ensembles de test:
![regular_2020_21](/figures/regular_2020_21.png)
![playoffs_2020_21](/figures/playoffs_2020_21.png)

**Saison Régulière**

| Modèle | AUC | Précision | Rappel | F1 |
|--------|----------|-----------|--------|--------|
| LR (distance) | 0.699031 | 0.000000 | 0.000000 | 0.000000 |
| LR (angle) | 0.559902 | 0.000000 | 0.000000 | 0.000000 |
| LR (distance+angle) | 0.715517 | 0.000000 | 0.000000 | 0.000000 |
| XGBoost (tuned+FS) | 0.567873 | 0.000000 | 0.000000 | 0.000000 |
| Ensemble (final) | 0.693603 | 0.317568 | 0.036321 | 0.065187 |
<br>

**Séries Éliminatoires (Playoffs)**

| Modèle | AUC | Précision | Rappel | F1 |
|--------|----------|-----------|--------|--------|
| LR (distance) | 0.675961 | 0.000000 | 0.000000 | 0.000000 |
| LR (angle) | 0.573355 | 0.000000 | 0.000000 | 0.000000 |
| LR (distance+angle) | 0.698033 | 0.000000 | 0.000000 | 0.000000 |
| XGBoost (tuned+FS) | 0.565428 | 0.000000 | 0.000000 | 0.000000 |
| Ensemble (final) | 0.677020 | 0.285714 | 0.026087 | 0.047809 |
<br>

**Comparaison des AUC**

| Modèle | Saison Régulière | Playoffs | Différence |
|--------|------------------|----------|------------|
| LR (distance) | 0.699031 | 0.675961 | -0.023070 |
| LR (angle) | 0.559902 | 0.573355 | +0.013453 |
| LR (distance+angle) | 0.715517 | 0.698033 | -0.017484 |
| XGBoost (tuned+FS) | 0.567873 | 0.565428 | -0.002445 |
| Ensemble (final) | 0.693603 | 0.677020 | -0.016583 |
<br>

## Performance sur la Saison Régulière 2020-2021
Les modèles montrent une bonne capacité de classement du danger des tirs (bons AUC, courbes ROC cohérentes). Les courbes par percentile et cumulatives confirment que les modèles ordonnent efficacement les tirs du plus au moins dangereux. <br>
Le modèle de régression logistique combinant distance et angle se démarque comme le plus performant, démontrant qu'une approche simple avec des features pertinentes surpasse les modèles plus complexes. De façon intéressante, le modèle qu'on pensait être le plus performant, XGBoost (avec des features sélectionnés de façon extensives et robustes pour ne que garder les 10 plus informatives), est en fait beaucoup moins performant qu'un modèle simple de régression logistique sur le test dataset. Ceci est probablement causé par un **sur-apprentissage** important : la chute dramatique de performance entre la validation (AUC = 0.745) et le test 2020-2021 (AUC = 0.568) révèle que le modèle a mémorisé des patterns spécifiques aux saisons 2016-2020 qui ne généralisent pas à 2020-2021. La complexité de XGBoost devient alors malheureusement un désavantage lorsque les prédicteurs clés (distance et angle) ont une relation surtout linéaire et stable avec la probabilité de but : le modèle capture des interactions non-linéaires complexes dans les 24 features qui sont probablement plus du bruit que du signal utile. La sélection de features basée sur `feature_importance` de XGBoost a probablement renforcé ce biais en privilégiant des variables à haute cardinalité qui semblent importantes dans les arbres mais contribuent négativement à la généralisation. De plus, le **déséquilibre extrême des classes (2-10% de buts)** (probablement encore moins de buts dans le test dataset que ceux d'entraînement/validation) et les particularités de la saison 2020-2021 (COVID-19) ont possiblement empiré la généralisation. Ainsi que pour ce problème spécifique, la simplicité et la robustesse de la régression logistique l'emportent sur la puissance théorique des arbres et des méthodes d'ensemble. <br>

Autre problème: tous les modèles sont **très mal calibrés**, et leurs probabilités restent bien en dessous d’un seuil de 0.5. Ceci témoigne d'une **sous-confiance** généralisée : lorsque les modèles prédisent une probabilité de 20%, le taux réel de buts est plus proche de 10-15%. De plus, tous les modèles sauf l'Ensemble affichent **Précision = Rappel = F1 = 0.000000**. Cela n'est probablement pas un signe de défaillance des modèles, mais plutôt qu'ils ne produisent **aucune prédiction positive avec le seuil standard de 0.5**. En combinant des modèles conservateurs (tel que démontré par les courbes de calibration) et la rareté accentuée des buts dans le dataset de test, leurs probabilités prédites restent systématiquement en dessous de 0.5.
Le modèle Ensemble échappe un peu à ce problème car il utilise probablement un mécanisme de vote ou un seuil adapté qui produit effectivement des prédictions positives (Précision = 0.318, Rappel = 0.036, F1 = 0.065).
Bref, les modèles fonctionnent bien pour classer le danger des tirs (bon AUC, bonnes courbes de percentile), mais échouent à produire des classifications utilisables en raison :
- D'un seuil de classification inapproprié (0.5 est trop élevé pour des événements rares)
- D'une mauvaise calibration des probabilités
- Du déséquilibre extrême des classes

## Performance sur les Séries Éliminatoires 2020-2021
Les résultats sur les séries éliminatoires sont quasi identiques à ceux de la saison régulière. Cette stabilité suggère que:
- Les modèles ont une capacité de généralisation acceptable pour l'année 2020-2021
- Les séries éliminatoires ne présentent pas de caractéristiques fondamentalement différentes en termes de prédiction de buts. 
- Il y a une certaine similarité du jeu entre saison régulière et séries éliminatoires pour ce qui concerne la géométrie des tirs (d'où la bonne performance des régressions logistiques utilisant la distance et l'angle)
<br>

De plus, les séries éliminatoires montrent des problèmes de calibration similaires, voire légèrement amplifiés, et des métriques de précision, rappel et F1 à zéro. 
<br>
<br>

# Conclusion générale
- Les modèles (surtout LR distance+angle) classent correctement le danger des tirs
- Un modèle simple (LR avec 2 features) surpasse les approches complexes
- Calibration déficiente : Les probabilités prédites ne reflètent pas les taux réels de buts
- Besoin d'ajuster les seuils pour tenir en compte du problème de débalancement des classes
<br>
<br>
<br>

# Modèles enregistrés dans le répertoire Wandb
-	Logistic Regression distance
-	Logistic Regression angle
-	Logistic Regression distance + angle
-	XGBoost (tuned with selected features)
-	Méthode Ensemble
